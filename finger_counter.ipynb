{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763551916.838740  515571 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1763551916.868363  515830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763551916.880672  515830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763551916.907683  515831 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mp_hands.Hands(min_detection_confidence=\u001b[32m0.5\u001b[39m, min_tracking_confidence=\u001b[32m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hands:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# Read frame from camera\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m         \u001b[38;5;66;03m# Convert image to RGB\u001b[39;00m\n\u001b[32m     21\u001b[39m         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pygame\n",
    "\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"sadarkan_aku.wav\")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Load video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Mediapipe hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        # Read frame from camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flip image horizontally\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Set flag to detect landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw landmarks on image\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)     \n",
    "        \n",
    "        # Detect finger count\n",
    "        finger_count = 0\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            tip_ids = [4, 8, 12, 16, 20]  # Landmark ids of finger tips\n",
    "            finger_states = []\n",
    "            for tip_id in tip_ids:\n",
    "                finger_tip = hand_landmarks.landmark[tip_id]\n",
    "                finger_mcp = hand_landmarks.landmark[tip_id - 1]\n",
    "                # Check if finger is open or closed\n",
    "                if tip_id==4:\n",
    "                    finger_states.append(finger_tip.x < finger_mcp.x)\n",
    "                else:\n",
    "                    finger_states.append(finger_tip.y < finger_mcp.y)\n",
    "            # Count number of open fingers\n",
    "            finger_count = finger_states.count(True)\n",
    "\n",
    "        # Display finger count on image (large, centered with outline for readability)\n",
    "        text = str(finger_count) if finger_count != 5 else \"JAM LIMA MENTIONED RAHHH\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        # Scale and thickness tuned for large display; adjust if it's too big/small for your camera resolution\n",
    "        scale = 2\n",
    "        thickness = 5\n",
    "        # Calculate text size so we can center it\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, font, scale, thickness)\n",
    "        x = (image.shape[1] - text_width) // 2\n",
    "        # y is baseline-based: put text vertically centered visually\n",
    "        y = (image.shape[0] + text_height) // 2\n",
    "        # Draw a thick black outline for contrast\n",
    "        cv2.putText(image, text, (x, y), font, scale, (0, 0, 0), thickness + 6, cv2.LINE_AA)\n",
    "        # Draw the colored text on top\n",
    "        cv2.putText(image, text, (x, y), font, scale, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "        # Display image\n",
    "        cv2.imshow('Finger Counter', image)\n",
    "\n",
    "        # Play music only while finger_count == 5, stop otherwise\n",
    "        if finger_count == 5:\n",
    "            if not pygame.mixer.music.get_busy():\n",
    "                pygame.mixer.music.play(-1)\n",
    "        else:\n",
    "            if pygame.mixer.music.get_busy():\n",
    "                pygame.mixer.music.stop()\n",
    "\n",
    "        # Check for 'q' key to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bed1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763600136.121340   15621 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1763600136.143353   16279 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763600136.158909   16279 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763600138.107972   16281 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-11-20 08:55:38.563 Python[1286:15621] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m image = cv2.flip(image, \u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Set flag to detect landmarks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m results = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Detect finger count\u001b[39;00m\n\u001b[32m     30\u001b[39m finger_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Grind/finger_counter_webcam/.venv/lib/python3.11/site-packages/mediapipe/python/solutions/hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Grind/finger_counter_webcam/.venv/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pygame\n",
    "\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"sadarkan_aku.wav\")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Load video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Mediapipe hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        # Read frame from camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flip image horizontally\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Set flag to detect landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Detect finger count\n",
    "        finger_count = 0\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            tip_ids = [4, 8, 12, 16, 20]  # Landmark ids of finger tips\n",
    "            finger_states = []\n",
    "            for tip_id in tip_ids:\n",
    "                finger_tip = hand_landmarks.landmark[tip_id]\n",
    "                finger_mcp = hand_landmarks.landmark[tip_id - 1]\n",
    "                # Check if finger is open or closed\n",
    "                if tip_id==4:\n",
    "                    finger_states.append(finger_tip.x < finger_mcp.x)\n",
    "                else:\n",
    "                    finger_states.append(finger_tip.y < finger_mcp.y)\n",
    "            # Count number of open fingers\n",
    "            finger_count = finger_states.count(True)\n",
    "\n",
    "        display_img = cv2.imread(\"/Users/abi/Documents/Grind/finger_counter_webcam/jam_lima.JPG\")\n",
    "        # Display finger count on image (large, centered with outline for readability)\n",
    "        text = str(finger_count) if finger_count != 5 else \"JAM LIMA MENTIONED RAHHH\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        # Scale and thickness tuned for large display; adjust if it's too big/small for your camera resolution\n",
    "        scale = 10\n",
    "        thickness = 30\n",
    "        # Calculate text size so we can center it\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, font, scale, thickness)\n",
    "        x = (display_img.shape[1] - text_width) // 2\n",
    "        # y is baseline-based: put text vertically centered visually\n",
    "        y = (display_img.shape[0] + text_height) // 2\n",
    "        # Draw a thick black outline for contrast\n",
    "        cv2.putText(display_img, text, (x, y), font, scale, (0, 0, 0), thickness + 6, cv2.LINE_AA)\n",
    "        # Draw the colored text on top\n",
    "        cv2.putText(display_img, text, (x, y), font, scale, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "        # Play music only while finger_count == 5, stop otherwise\n",
    "        if finger_count == 5:\n",
    "            if not pygame.mixer.music.get_busy():\n",
    "                pygame.mixer.music.play(-1)\n",
    "            # Display image\n",
    "            cv2.imshow('JAM LIMA', display_img)\n",
    "        else:\n",
    "            if pygame.mixer.music.get_busy():\n",
    "                pygame.mixer.music.stop()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        # Check for 'q' key to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c49abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
